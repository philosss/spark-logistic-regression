{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test done. Train contains 3717 elements, Test contains 884 elements\n",
      "-> Iteration done: 1 of 5. Cost: 0.6429051336857715\n",
      "-> Iteration done: 2 of 5. Cost: 0.6116104380482642\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "global k_fold\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "TRAIN_TEST = 0.8\n",
    "k_fold=5\n",
    "\n",
    "global n_columns\n",
    "global w\n",
    "global b\n",
    "global n_row\n",
    "global y_h_m_y\n",
    "\n",
    "\n",
    "n_columns=56 #total number of columns (58) - the one deleted (57) - label (56)\n",
    "col_sums=[]\n",
    "averages=[]\n",
    "sigmas=[]\n",
    "w=[]\n",
    "b=0\n",
    "\n",
    "\n",
    "\n",
    "def initializeAccumulators():\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        averages.append(sc.accumulator(0))\n",
    "        col_sums.append(sc.accumulator(0))\n",
    "        sigmas.append(sc.accumulator(0)) \n",
    "        i+=1\n",
    "\n",
    "def addToAccumulators(row):\n",
    "    if(len(row)!=len(col_sums)):\n",
    "        raise Exception(\"Number of columns in the row doesn't mach the number of accumulators initiated. Len row: \"+str(len(row))+\" n_accomulators\"+str(len(col_sums)))\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        print(\"Len row: \"+str(len(row))+\" n_accomulators\"+str(len(col_sums)))\n",
    "        col_sums[i].add(row[i])\n",
    "        i+=1\n",
    "        \n",
    "def preprocessing(row):\n",
    "    #splitting\n",
    "    split=row.split(\" \")\n",
    "    label = int(split[57])\n",
    "    del split[57] #label\n",
    "    del split[56] #col 57th\n",
    "    split=[float(col) for col in split]\n",
    "    addToAccumulators(split)\n",
    "    #assign random key to the datapoint\n",
    "    key=random.getrandbits(64)\n",
    "    #assign train/test \n",
    "    if(random.random()<TRAIN_TEST):\n",
    "        train=1\n",
    "    else:\n",
    "        train=0    \n",
    "    #return\n",
    "    return (key,train,split,label)\n",
    "def calcAvg(n_row):\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        averages[i]=col_sums[i].value/n_row\n",
    "        i+=1\n",
    "def calcResiduals(row):\n",
    "    i=0\n",
    "    while(i<n_columns):       \n",
    "        sigmas[i].add(math.pow(row[2][i]-averages[i],2))\n",
    "        i+=1\n",
    "    return row\n",
    "def calcSigmas(n_row):\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        sigmas[i]=math.sqrt(sigmas[i].value/float(n_row-1))\n",
    "        i+=1\n",
    "def normalize(row):\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        row[2][i]=(row[2][i]-averages[i])/sigmas[i]\n",
    "        i+=1\n",
    "    return row\n",
    "\n",
    "def initializeWeights(random_init=False):\n",
    "    if(random_init):\n",
    "        #return sc.parallelize([(i, random.random()) for i in range(0,n_columns)])\n",
    "        return [random.random() for i in range(0,n_columns)]\n",
    "    else:\n",
    "        #return sc.parallelize([(i, 0.0) for i in range(0,n_columns)])\n",
    "        return [0.0 for i in range(0,n_columns)]\n",
    "\n",
    "def initializeBias():\n",
    "    return sc.parallelize(0.0)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1/(1+math.exp(-z))\n",
    "\n",
    "def predict(w,b,X):\n",
    "    return sigmoid(sum([X[i]*w[i] for i in range(len(w))])+b)\n",
    "\n",
    "def predict_parallel(w,b,X):\n",
    "    #Change X representation\n",
    "    X = sc.parallelize(X).zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    wX_plus_b=X.join(w).map(lambda x: (x[1][0]*x[1][1])+b).sum()\n",
    "    return sigmoid(wX_plus_b)\n",
    "\n",
    "def compute_cost(dataset,w,b,lambda_reg):\n",
    "    return (-1/dataset.count())*dataset.map(lambda x: x[3]*math.log(predict(w,b,x[2])) \\\n",
    "                                    +(1-x[3])*math.log(1-predict(w,b,x[2]))).sum() \\\n",
    "                                    + lambda_reg/(2*dataset.count())*sum([i*i for i in w]);\n",
    "\n",
    "def make_folds(x):\n",
    "    return (x[0],int(random.random()*10-1),x[1],x[2])\n",
    "     \n",
    "    \n",
    "def train(filename, iterations, learning_rate, lambda_reg):\n",
    "    global w\n",
    "    global b\n",
    "    initializeAccumulators()\n",
    "    dataset=sc.textFile(filename).map(preprocessing).sortBy(lambda x: x[0])\n",
    "    n_row=dataset.count()\n",
    "#    print(dataset.first())\n",
    "    calcAvg(n_row)\n",
    "    dataset.map(calcResiduals).collect()\n",
    "    calcSigmas(n_row)\n",
    "    dataset=dataset.map(normalize)\n",
    "#    print(dataset.first())\n",
    "    train=dataset.filter(lambda x: x[1]==1)\n",
    "    train_size=train.count()\n",
    "#    train_x=train.map(lambda x: x[2])\n",
    "#    train_y=train.map(lambda x: x[3])\n",
    "    test=dataset.filter(lambda x: x[1]==0)\n",
    "#    test_x=test.map(lambda x: x[2])\n",
    "#    test_y=test.map(lambda x: x[3])\n",
    "    print(\"Split train/test done. Train contains \"+str(train.count())+\" elements, Test contains \"+str(test.count())+\" elements\")\n",
    "    fold_length=train.count()/k_fold\n",
    "    #Folds making\n",
    "    costs_fold=[]\n",
    "    for i_fold in range(k_fold):\n",
    "        starting_fold=fold_length*i_fold\n",
    "        end_fold=starting_fold+fold_length\n",
    "        test_fold=train.zipWithIndex().filter(lambda t: (t[1]>=starting_fold and t[1]<end_fold)).map(lambda t: t[0]) #the map get rid of the index again\n",
    "        \n",
    "        train_fold=train.zipWithIndex().filter(lambda t: t[1]<starting_fold or t[1]>=end_fold).map(lambda t: t[0])\n",
    "        train_fold_size=train_fold.count()\n",
    "        #print(\"Test fold:\")\n",
    "        #print(test_fold.take(1))\n",
    "        #print(\"Train fold:\")\n",
    "        #print(train_fold.take(1))\n",
    "        w=initializeWeights()\n",
    "        dw=[i for i in range(0,n_columns)]\n",
    "        #Gradient descent\n",
    "        costs=[]\n",
    "        for iteration in range(iterations):\n",
    "            for j in range(n_columns):\n",
    "                X_j=train_fold.map(lambda x: (predict(w,b,x[2])-x[3])*x[2][j]).sum()\n",
    "                dw[j]=(1/train_fold_size)*X_j+(lambda_reg/train_fold_size)+w[j]\n",
    "                w[j]-=learning_rate*dw[j]\n",
    "            b-=learning_rate*(1/train_fold_size)*train_fold.map(lambda x: predict(w,b,x[2])-x[3]).sum()\n",
    "            cost=compute_cost(train_fold,w,b,lambda_reg)\n",
    "            costs.append(cost)\n",
    "            print(\"-> Iteration done: \"+str(iteration+1)+\" of \"+str(iterations)+\". Cost: \"+str(cost))\n",
    "        costs_fold.append(costs)\n",
    "        print(\"--> Fold #\"+str(i_fold+1)+\" of \"+str(k_fold)+\" is done. Average cost: \"+str(sum(costs)/iterations))\n",
    "    \n",
    "    costs_fold=list(chain.from_iterable(costs_fold))\n",
    "    print(\"---> \"+str(k_fold)+\"-fold validation done. Average cost: \"+str(sum(costs_fold)/(iterations*k_fold)))\n",
    "            \n",
    "    \n",
    "    \n",
    "train(\"dataset/spam.data\",5,0.1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predict(w,b,X):\n",
    "    #Change X representation\n",
    "    X = X.zipWithIndex().map(lambda x: (x[1],x[0]))\n",
    "    wX_plus_b=X.join(w).map(lambda x: (x[1][0]*x[1][1])+b).sum()\n",
    "    return sigmoid(wX_plus_b)\n",
    "predict(initializeWeights(), 0, sc.parallelize([i for i in range(n_columns)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-84-a4c43414cb95>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-84-a4c43414cb95>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    for()\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def gradientDecent(w,iterations,learning_rate,lambda_reg):\n",
    "    dW=sc.parallelize([0 for i in range(n_columns)])\n",
    "    \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filename' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a22096a174a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minitializeAccumulators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mn_row\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#    print(dataset.first())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filename' is not defined"
     ]
    }
   ],
   "source": [
    "#t=zip([] for i in range(0,n_columns),\"w\"+str(i) for i in range(0,n_columns))\n",
    "t=set(zip([i for i in range(0,n_columns)],[i for i in range(0,n_columns)]))\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose(x,y):\n",
    "    for i in range(0,k):\n",
    "        x[0][i]+=y[i]\n",
    "train.aggregate(([[] for i in range(0,n_columns)]],[]), transpose, globalize)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.02969887985785198,\n",
       " -0.008284080268595848,\n",
       " 0.041822455051646536,\n",
       " 0.017256782364696212,\n",
       " 0.05326246112325493,\n",
       " 0.05525192221253085,\n",
       " 0.08359049881679019,\n",
       " 0.05357137573372938,\n",
       " 0.05334075512061355,\n",
       " 0.032029808065755165,\n",
       " 0.05258743991967035,\n",
       " 0.0074777587120948945,\n",
       " 0.030843601386010843,\n",
       " 0.015026309504026104,\n",
       " 0.04391438471523303,\n",
       " 0.06214018080185739,\n",
       " 0.058092914082291607,\n",
       " 0.04612961025274813,\n",
       " 0.037837685771370554,\n",
       " 0.04307990244029978,\n",
       " 0.06509162065659488,\n",
       " 0.027069961330382444,\n",
       " 0.07540793938931052,\n",
       " 0.051334684864959006,\n",
       " -0.04792783878428576,\n",
       " -0.04535069147091169,\n",
       " -0.04060001080916476,\n",
       " -0.02602467970098737,\n",
       " -0.02396264602748511,\n",
       " -0.030323902587659126,\n",
       " -0.020458479099227208,\n",
       " -0.017709351797302927,\n",
       " -0.029095141822853526,\n",
       " -0.01674092615162573,\n",
       " -0.02503572963510947,\n",
       " -0.02085794067984527,\n",
       " -0.033570272590953215,\n",
       " -0.009213712049893038,\n",
       " -0.025080441292352786,\n",
       " -0.005469604052854424,\n",
       " -0.02086594317751748,\n",
       " -0.029687397143216786,\n",
       " -0.028632641677854224,\n",
       " -0.021555052906557046,\n",
       " -0.028249578925156896,\n",
       " -0.032357162404522646,\n",
       " -0.01159447384712299,\n",
       " -0.020739572961591544,\n",
       " -0.01553545742957214,\n",
       " -0.0073017163957651,\n",
       " -0.011542208808445871,\n",
       " 0.05726115638480555,\n",
       " 0.07362008964793323,\n",
       " 0.01744948677896831,\n",
       " 0.027531356264287727,\n",
       " 0.04814953020883681]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
