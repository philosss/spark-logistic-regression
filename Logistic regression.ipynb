{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "TRAIN_TEST = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "global n_columns\n",
    "n_columns=56 #total number of columns - the one deleted - the target = 56\n",
    "col_sums=[]\n",
    "averages=[]\n",
    "sigmas=[]\n",
    "\n",
    "\n",
    "def initializeAccumulators():\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        averages.append(sc.accumulator(0))\n",
    "        col_sums.append(sc.accumulator(0))\n",
    "        sigmas.append(sc.accumulator(0)) \n",
    "        i+=1\n",
    "\n",
    "def addToAccumulators(row):\n",
    "    if(len(row)-1!=len(col_sums)):\n",
    "        raise Exception(\"Number of columns in the row doesn't mach the number of accumulators initiated. Len row: \"+str(len(row))+\" n_accomulators\"+str(len(col_sums)))\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        print(\"Len row: \"+str(len(row))+\" n_accomulators\"+str(len(col_sums)))\n",
    "        col_sums[i].add(row[i])\n",
    "        i+=1\n",
    "        \n",
    "def preprocessing(row):\n",
    "    #splitting\n",
    "    split=row.split(\" \")\n",
    "    #del 56 col\n",
    "    del split[56]\n",
    "    split=[float(col) for col in split]\n",
    "    addToAccumulators(split)\n",
    "    #assign random key to the datapoint\n",
    "    key=random.getrandbits(64)\n",
    "    #assign train/test \n",
    "    if(random.random()<TRAIN_TEST):\n",
    "        train=1\n",
    "    else:\n",
    "        train=0    \n",
    "    #return\n",
    "    return (key,train,split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializeAccumulators()\n",
    "dataset=sc.textFile(\"dataset/spam.data\").map(preprocessing).sortBy(lambda x: x[0])\n",
    "n_row=dataset.count();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcAvg():\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        averages[i]=col_sums[i].value/n_row\n",
    "        i+=1\n",
    "#Average\n",
    "calcAvg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcResiduals(row):\n",
    "    i=0\n",
    "    while(i<n_columns):       \n",
    "        sigmas[i].add(math.pow(row[2][i]-averages[i],2))\n",
    "        i+=1\n",
    "    return row\n",
    "def calcSigmas():\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        sigmas[i]=math.sqrt(sigmas[i].value/float(n_row-1))\n",
    "        i+=1\n",
    "def normalize(row):\n",
    "    i=0\n",
    "    while(i<n_columns):\n",
    "        row[2][i]=(row[2][i]-averages[i])/sigmas[i]\n",
    "        i+=1\n",
    "    return row\n",
    "        \n",
    "        \n",
    "dataset.map(calcResiduals).collect()\n",
    "calcSigmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#we normalize!\n",
    "dataset=dataset.map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split train/test done. Train contains 3689 elements, Test contains 912 elements\n"
     ]
    }
   ],
   "source": [
    "train=dataset.filter(lambda x: x[1]==1)\n",
    "test=dataset.filter(lambda x: x[1]==0)\n",
    "print(\"Split train/test done. Train contains \"+str(train.count())+\" elements, Test contains \"+str(test.count())+\" elements\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_RDD=sc.parallelize(dataset)\n",
    "X=sc.parallelize(dataset.drop(\"f57\", axis=1))\n",
    "y=sc.parallelize(dataset[\"f57\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X=preprocessing.normalize(dataset.drop(\"f57\", axis=1), norm='l2')\n",
    "y=dataset[\"f57\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
